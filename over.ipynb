{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV6ycAzN1U6h"
   },
   "source": [
    "## LOAD DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pscRG6416YHa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nckh1/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JD4SiJCm1thD"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_over.csv')\n",
    "\n",
    "data2 = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzJJ4H9jXURJ"
   },
   "source": [
    "## BUILDING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3FO73jPXXYa1"
   },
   "outputs": [],
   "source": [
    "# divide test set and training set\n",
    "\n",
    "X_train = data.drop(\"15c3d_5\", axis= 1)\n",
    "y_train = data['15c3d_5']\n",
    "\n",
    "X_test = data2.drop(\"15c3d_5\", axis=1)\n",
    "y_test = data2['15c3d_5']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCrDXOcjVoa0"
   },
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# rus = RandomUnderSampler(random_state=0)\n",
    "# rus.fit(X, y)\n",
    "# X_resampled, y_resampled = rus.sample(X, y)\n",
    "# colors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled]\n",
    "# plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, linewidth=0.5, edgecolor='black')\n",
    "# sns.despine()\n",
    "# plt.title(\"RandomUnderSampler Output ($n_{class}=64)$\")\n",
    "# pass\n",
    "\n",
    "\n",
    "\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "# # fit and apply the transform\n",
    "# X_train, y_train = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxqRPOByVqxy",
    "outputId": "379cfc87-2a0a-4e55-aa45-309f795eb3b7"
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 42)\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size= 0.2, random_state= 42)\n",
    "\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeSoxcuwXf-O"
   },
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4jzKHL4dYi55",
    "outputId": "2ecb82d1-523b-4c1e-cda7-cc2a09ed09b5"
   },
   "outputs": [],
   "source": [
    "#t5\n",
    "logreg=LogisticRegression(max_iter=10000)\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "logreg.fit(X_train, y_train) \n",
    "score = logreg.score(X_train, y_train)\n",
    "score2 = logreg.score(X_test, y_test)\n",
    "\n",
    "print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORi0eCAkYjyI"
   },
   "outputs": [],
   "source": [
    "#t6\n",
    "# logreg=LogisticRegression(max_iter=10000)\n",
    "# logreg.fit(X_train2,y_train2)\n",
    "\n",
    "# logreg.fit(X_train2, y_train2) \n",
    "# score = logreg.score(X_train2, y_train2)\n",
    "# score2 = logreg.score(X_test2, y_test2)\n",
    "\n",
    "# print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "# print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "# y_pred2 = logreg.predict(X_test2)\n",
    "# print(confusion_matrix(y_test2,y_pred2))\n",
    "# print(classification_report(y_test2,y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJo14NFRZp4m"
   },
   "source": [
    "### XGBOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-T5T52kaLfz"
   },
   "outputs": [],
   "source": [
    "#T5\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_model.fit(X_train, y_train) \n",
    "score = xgb_model.score(X_train, y_train)\n",
    "score2 = xgb_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRn-GmBlaMLp"
   },
   "outputs": [],
   "source": [
    "# T6\n",
    "\n",
    "# xgb_model = XGBClassifier()\n",
    "# xgb_model.fit(X_train2, y_train2)\n",
    "\n",
    "# xgb_model.fit(X_train2, y_train2) \n",
    "# score = xgb_model.score(X_train2, y_train2)\n",
    "# score2 = xgb_model.score(X_test2, y_test2)\n",
    "\n",
    "# print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "# print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "# y_pred2 = xgb_model.predict(X_test2)\n",
    "# print(confusion_matrix(y_test2,y_pred2))\n",
    "# print(classification_report(y_test2,y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQcGrkGyarD-"
   },
   "source": [
    "### GRADIENT BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7wJhkVIav-0"
   },
   "outputs": [],
   "source": [
    "#t5\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=125,max_depth=5)\n",
    "gb.fit(X_train, y_train) \n",
    "score = gb.score(X_train, y_train)\n",
    "score2 = gb.score(X_test, y_test)\n",
    "\n",
    "print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "y_pred = gb.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3tKj4MZawOZ"
   },
   "outputs": [],
   "source": [
    "#t6\n",
    "\n",
    "# gb = GradientBoostingClassifier(n_estimators=125,max_depth=5)\n",
    "# gb.fit(X_train2, y_train2) \n",
    "# score = gb.score(X_train2, y_train2)\n",
    "# score2 = gb.score(X_test2, y_test2)\n",
    "\n",
    "# print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "# print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "# y_pred2 = gb.predict(X_test2)\n",
    "# print(confusion_matrix(y_test2,y_pred2))\n",
    "# print(classification_report(y_test2,y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7JaFnIOb3HQ"
   },
   "source": [
    "### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MLqnb-fb5y9"
   },
   "outputs": [],
   "source": [
    "#t5\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1200, max_depth=30, n_jobs =-1, min_samples_leaf=1,\n",
    "                                min_samples_split= 10, max_features = 'auto', criterion = 'entropy', bootstrap= True,\n",
    "                              random_state=42)\n",
    "rf.fit(X_train, y_train) \n",
    "score = rf.score(X_train, y_train)\n",
    "score2 = rf.score(X_test, y_test)\n",
    "\n",
    "print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4voo9LWKb6Eu"
   },
   "outputs": [],
   "source": [
    "#t6\n",
    "\n",
    "# rf = RandomForestClassifier(n_estimators=1200, max_depth=80, n_jobs =-1, min_samples_leaf=1,\n",
    "#                                 min_samples_split= 10, max_features = 'auto', criterion = 'entropy', bootstrap= True,\n",
    "#                               random_state=42)\n",
    "# rf.fit(X_train2, y_train2) \n",
    "# score = rf.score(X_train2, y_train2)\n",
    "# score2 = rf.score(X_test2, y_test2)\n",
    "\n",
    "# print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "# print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "# y_pred2 = rf.predict(X_test2)\n",
    "# print(confusion_matrix(y_test2,y_pred2))\n",
    "# print(classification_report(y_test2,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train,y_train = shuffle(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qawiSiJddQXm"
   },
   "source": [
    "### DEEP NEURAL DECISION FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9XAEQ8WAdX1C"
   },
   "outputs": [],
   "source": [
    "data_train = pd.concat([X_train, y_train], axis=1)\n",
    "data_train['15c3d_5'] = data_train['15c3d_5'].astype(str)\n",
    "data_train['15c3d_5'].replace([\"0\", \"1\"], [\"y\", \"n\"], inplace= True)\n",
    "\n",
    "data_test = pd.concat([X_test, y_test], axis=1)\n",
    "data_test['15c3d_5'] = data_test['15c3d_5'].astype(str)\n",
    "data_test['15c3d_5'].replace([\"0\", \"1\"], [\"y\", \"n\"], inplace= True)\n",
    "\n",
    "# data_train2 = pd.concat([X_train2, y_train2], axis=1)\n",
    "# data_train2['15c3d_6'] = data_train2['15c3d_6'].astype(str)\n",
    "# data_train2['15c3d_6'].replace([\"0\", \"1\"], [\"y\", \"n\"], inplace= True)\n",
    "\n",
    "# data_test2 = pd.concat([X_test2, y_test2], axis=1)\n",
    "# data_test2['15c3d_6'] = data_test2['15c3d_6'].astype(str)\n",
    "# data_test2['15c3d_6'].replace([\"0\", \"1\"], [\"y\", \"n\"], inplace= True)\n",
    "\n",
    "data_train.to_csv(\"dndf_over.csv\", index=False, header=False)\n",
    "data_test.to_csv(\"dndf_test.csv\", index=False, header=False)\n",
    "\n",
    "# data_train2.to_csv(\"train2.csv\", index=False, header=False)\n",
    "# data_test2.to_csv(\"test2.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7WZ_unlmgjUL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              AGE      TD_2      TD_3      TD_4    DATA_2    DATA_3    DATA_4  \\\n",
      "1806751  0.184063  0.010912  0.002366  0.002573  0.017142  0.004417  0.006018   \n",
      "2474977  0.411149  0.004178  0.012737  0.011577  0.000063  0.004352  0.000321   \n",
      "597225   0.121212  0.000000  0.000000  0.002547  0.000000  0.000000  0.000706   \n",
      "3659758  0.049383  0.000000  0.016562  0.015436  0.000000  0.015851  0.015937   \n",
      "3713538  0.279461  0.000000  0.000000  0.002573  0.000025  0.000026  0.000552   \n",
      "\n",
      "         ONNET_IN_2  ONNET_IN_3  ONNET_IN_4  ...  RC_MONEY_4  REG_ON_2  \\\n",
      "1806751    0.001185    0.000665    0.002093  ...    0.006667  1.000000   \n",
      "2474977    0.004283    0.009821    0.002968  ...    0.034667  1.000000   \n",
      "597225     0.000259    0.000037    0.000937  ...    0.006667  0.035714   \n",
      "3659758    0.000000    0.006720    0.029070  ...    0.040000  0.000000   \n",
      "3713538    0.000000    0.000000    0.000493  ...    0.006667  1.000000   \n",
      "\n",
      "         REG_ON_3  REG_ON_4  15c3d_2  15c3d_3  15c3d_4  OS_FF  OS_SM  15c3d_5  \n",
      "1806751  1.000000  1.000000        1        1        1      0      1        y  \n",
      "2474977  1.000000  1.000000        1        1        1      0      1        n  \n",
      "597225   0.096774  0.133333        0        0        1      0      1        n  \n",
      "3659758  0.645161  0.900000        0        1        1      0      0        y  \n",
      "3713538  0.967742  1.000000        0        0        1      0      1        y  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_train.head())\n",
    "# print(data_test2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_wp502_4dYY8"
   },
   "outputs": [],
   "source": [
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = data_train.drop(\"15c3d_5\", axis=1).columns\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \n",
    "}\n",
    "# A list of the columns to ignore from the dataset.\n",
    "#IGNORE_COLUMN_NAMES = [\"fnlwgt\"]\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = []\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES else [\"NA\"]\n",
    "    for feature_name in data_train.columns\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"15c3d_5\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\"y\", \"n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BvabIk0mdYsE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nckh1/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py:2446: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n",
      "2022-03-03 14:20:41.374171: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-03-03 14:20:41.374276: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-03-03 14:20:41.377241: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "target_label_lookup = StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=data_train.columns,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        na_value=\" \",\n",
    "        shuffle=shuffle,\n",
    "    ).map(lambda features, target: (features, target_label_lookup(target)))\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Dq8UvumOdYuu"
   },
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "q428nXfFdYyk"
   },
   "outputs": [],
   "source": [
    "def encode_inputs(inputs):\n",
    "    encoded_features = []\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "            # Create a lookup to convert a string values to an integer indices.\n",
    "            # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "            lookup = StringLookup(\n",
    "                vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n",
    "            )\n",
    "            # Convert the string input values into integer indices.\n",
    "            value_index = lookup(inputs[feature_name])\n",
    "            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims\n",
    "            )\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_feature = embedding(value_index)\n",
    "        else:\n",
    "            # Use the numerical features as-is.\n",
    "            encoded_feature = inputs[feature_name]\n",
    "            if inputs[feature_name].shape[-1] is None:\n",
    "                encoded_feature = tf.expand_dims(encoded_feature, -1)\n",
    "\n",
    "        encoded_features.append(encoded_feature)\n",
    "\n",
    "    encoded_features = layers.concatenate(encoded_features)\n",
    "    return encoded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2th6VHN1fuHM"
   },
   "outputs": [],
   "source": [
    "class NeuralDecisionTree(keras.Model):\n",
    "    def __init__(self, depth, num_features, used_features_rate, num_classes):\n",
    "        super(NeuralDecisionTree, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.num_leaves = 2 ** depth\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Create a mask for the randomly selected features.\n",
    "        num_used_features = int(num_features * used_features_rate)\n",
    "        one_hot = np.eye(num_features)\n",
    "        sampled_feature_indicies = np.random.choice(\n",
    "            np.arange(num_features), num_used_features, replace=False\n",
    "        )\n",
    "        self.used_features_mask = one_hot[sampled_feature_indicies]\n",
    "\n",
    "        # Initialize the weights of the classes in leaves.\n",
    "        self.pi = tf.Variable(\n",
    "            initial_value=tf.random_normal_initializer()(\n",
    "                shape=[self.num_leaves, self.num_classes]\n",
    "            ),\n",
    "            dtype=\"float32\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # Initialize the stochastic routing layer.\n",
    "        self.decision_fn = layers.Dense(\n",
    "            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n",
    "        )\n",
    "\n",
    "    def call(self, features):\n",
    "        batch_size = tf.shape(features)[0]\n",
    "\n",
    "        # Apply the feature mask to the input features.\n",
    "        features = tf.matmul(\n",
    "            features, self.used_features_mask, transpose_b=True\n",
    "        )  # [batch_size, num_used_features]\n",
    "        # Compute the routing probabilities.\n",
    "        decisions = tf.expand_dims(\n",
    "            self.decision_fn(features), axis=2\n",
    "        )  # [batch_size, num_leaves, 1]\n",
    "        # Concatenate the routing probabilities with their complements.\n",
    "        decisions = layers.concatenate(\n",
    "            [decisions, 1 - decisions], axis=2\n",
    "        )  # [batch_size, num_leaves, 2]\n",
    "\n",
    "        mu = tf.ones([batch_size, 1, 1])\n",
    "\n",
    "        begin_idx = 1\n",
    "        end_idx = 2\n",
    "        # Traverse the tree in breadth-first order.\n",
    "        for level in range(self.depth):\n",
    "            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n",
    "            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n",
    "            level_decisions = decisions[\n",
    "                :, begin_idx:end_idx, :\n",
    "            ]  # [batch_size, 2 ** level, 2]\n",
    "            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n",
    "            begin_idx = end_idx\n",
    "            end_idx = begin_idx + 2 ** (level + 1)\n",
    "\n",
    "        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n",
    "        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n",
    "        outputs = tf.matmul(mu, probabilities)  # [batch_size, num_classes]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "I8p-tFlpfuKN"
   },
   "outputs": [],
   "source": [
    "class NeuralDecisionForest(keras.Model):\n",
    "    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n",
    "        super(NeuralDecisionForest, self).__init__()\n",
    "        self.ensemble = []\n",
    "        # Initialize the ensemble by adding NeuralDecisionTree instances.\n",
    "        # Each tree will have its own randomly selected input features to use.\n",
    "        for _ in range(num_trees):\n",
    "            self.ensemble.append(\n",
    "                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
    "            )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = tf.zeros([batch_size, num_classes])\n",
    "\n",
    "        # Aggregate the outputs of trees in the ensemble.\n",
    "        for tree in self.ensemble:\n",
    "            outputs += tree(inputs)\n",
    "        # Divide the outputs by the ensemble size to get the average.\n",
    "        outputs /= len(self.ensemble)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8WMGC2wRfuOI"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "hidden_units = [64, 64]\n",
    "\n",
    "\n",
    "def run_experiment(model):\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    train_dataset = get_dataset_from_csv(\n",
    "        \"dndf_over.csv\", shuffle=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    model.fit(train_dataset, epochs=num_epochs)\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    print(\"Evaluating the model on the test data...\")\n",
    "    test_dataset = get_dataset_from_csv(\"dndf_test.csv\", batch_size=batch_size)\n",
    "\n",
    "    _, accuracy = model.evaluate(test_dataset)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XxNZ1mwdgCms"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/5\n",
      "31577/31577 [==============================] - 299s 9ms/step - loss: 0.4340 - sparse_categorical_accuracy: 0.7985\n",
      "Epoch 2/5\n",
      "31577/31577 [==============================] - 206s 7ms/step - loss: 0.4252 - sparse_categorical_accuracy: 0.8033\n",
      "Epoch 3/5\n",
      "31577/31577 [==============================] - 195s 6ms/step - loss: 0.4222 - sparse_categorical_accuracy: 0.8051\n",
      "Epoch 4/5\n",
      "31577/31577 [==============================] - 192s 6ms/step - loss: 0.4206 - sparse_categorical_accuracy: 0.8060\n",
      "Epoch 5/5\n",
      "31577/31577 [==============================] - 188s 6ms/step - loss: 0.4193 - sparse_categorical_accuracy: 0.8068\n",
      "Model training finished\n",
      "Evaluating the model on the test data...\n",
      "21883/21883 [==============================] - 131s 6ms/step - loss: 0.4626 - sparse_categorical_accuracy: 0.7771\n",
      "Test accuracy: 77.71%\n"
     ]
    }
   ],
   "source": [
    "num_trees = 25\n",
    "depth = 5\n",
    "used_features_rate = 0.5\n",
    "num_classes = len(TARGET_LABELS)\n",
    "\n",
    "def create_forest_model():\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "    features = layers.Dense(32,  activation ='relu'\n",
    "               )(features)\n",
    "#     features = layers.Dense(16, activation='relu')(features)\n",
    "#     features = layers.BatchNormalization()(features)\n",
    "    num_features = features.shape[1]\n",
    "\n",
    "    forest_model = NeuralDecisionForest(\n",
    "        num_trees, depth, num_features, used_features_rate, num_classes\n",
    "    )\n",
    "\n",
    "    outputs = forest_model(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "forest_model = create_forest_model()\n",
    "\n",
    "run_experiment(forest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.84      0.68    776389\n",
      "           1       0.93      0.76      0.84   2024530\n",
      "\n",
      "    accuracy                           0.78   2800919\n",
      "   macro avg       0.75      0.80      0.76   2800919\n",
      "weighted avg       0.83      0.78      0.79   2800919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = get_dataset_from_csv(\"dndf_test.csv\", batch_size=batch_size)\n",
    "aa = forest_model.predict(test_dataset)\n",
    "y_pred = []\n",
    "i = 0\n",
    "for a in aa:\n",
    "  if a[0] > a[1]:\n",
    "    y_pred.append(0)\n",
    "  else:\n",
    "    y_pred.append(1)\n",
    "  i = i + 1\n",
    "y_pred = np.array(y_pred)\n",
    "print(y_pred)\n",
    "y_test = data_test['15c3d_5']\n",
    "y_test.replace([\"y\", \"n\"], [\"0\", \"1\"], inplace= True)\n",
    "y_test = y_test.astype(np.int64)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bản sao của Bản sao của T+1_smote.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
