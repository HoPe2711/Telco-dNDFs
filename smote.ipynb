{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV6ycAzN1U6h"
   },
   "source": [
    "## LOAD DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pscRG6416YHa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nckh1/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JD4SiJCm1thD"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_smote.csv')\n",
    "\n",
    "data2 = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBikviuH2W7p",
    "outputId": "ac0da994-37c8-4db8-9b27-2b3653850672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        AGE      TD_2      TD_3      TD_4    DATA_2    DATA_3    DATA_4  \\\n",
      "0  0.054994  0.000000  0.009464  0.000000  0.000276  0.004625  0.000000   \n",
      "1  0.057613  0.000715  0.001012  0.000798  0.000589  0.000013  0.000026   \n",
      "2  0.092780  0.000149  0.004604  0.000000  0.000000  0.000156  0.000000   \n",
      "3  0.049009  0.000000  0.007098  0.007718  0.000000  0.005340  0.012421   \n",
      "4  0.548822  0.000000  0.004732  0.010291  0.001541  0.003066  0.008059   \n",
      "\n",
      "   ONNET_IN_2  ONNET_IN_3  ONNET_IN_4  ...  RC_MONEY_4  REG_ON_2  REG_ON_3  \\\n",
      "0    0.000637    0.004552    0.000088  ...    0.000000  0.142857  0.935484   \n",
      "1    0.001596    0.000125    0.000115  ...    0.000000  0.285714  0.419355   \n",
      "2    0.000025    0.000007    0.000000  ...    0.000000  0.071429  0.258065   \n",
      "3    0.000000    0.003432    0.004011  ...    0.020000  0.000000  0.645161   \n",
      "4    0.000185    0.000366    0.000322  ...    0.026667  1.000000  1.000000   \n",
      "\n",
      "   REG_ON_4  15c3d_2  15c3d_3  15c3d_4  OS_FF  OS_SM  15c3d_5  \n",
      "0  0.000000        0        1        0      0      1        0  \n",
      "1  0.733333        1        1        1      0      1        0  \n",
      "2  0.066667        0        1        0      1      0        1  \n",
      "3  1.000000        0        1        1      0      1        1  \n",
      "4  1.000000        0        1        1      0      0        1  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "        AGE      TD_2      TD_3      TD_4    DATA_2    DATA_3    DATA_4  \\\n",
      "0  0.075225  0.004732  0.000515  0.003252  0.004963  0.000693  0.003502   \n",
      "1  0.330090  0.003029  0.005166  0.004065  0.002274  0.002233  0.005880   \n",
      "2  0.046033  0.000000  0.005145  0.020325  0.001338  0.002939  0.008330   \n",
      "3  0.046033  0.000000  0.002573  0.000000  0.000000  0.000629  0.000000   \n",
      "4  0.279940  0.005671  0.003659  0.008497  0.000000  0.000000  0.000000   \n",
      "\n",
      "   ONNET_IN_2  ONNET_IN_3  ONNET_IN_4  ...  RC_MONEY_4  REG_ON_2  REG_ON_3  \\\n",
      "0    0.000552    0.000016    0.000086  ...    0.003971  1.000000  1.000000   \n",
      "1    0.003905    0.001278    0.004906  ...    0.003971  0.967742  1.000000   \n",
      "2    0.000107    0.001441    0.000180  ...    0.019857  0.387097  1.000000   \n",
      "3    0.000033    0.001107    0.000000  ...    0.000000  0.193548  0.233333   \n",
      "4    0.002709    0.002120    0.001118  ...    0.007943  0.967742  1.000000   \n",
      "\n",
      "   REG_ON_4  15c3d_2  15c3d_3  15c3d_4  15c3d_5  OS_FF  OS_SM  \n",
      "0  0.903226        1        1        1        1      0      1  \n",
      "1  1.000000        1        1        1        1      0      1  \n",
      "2  0.967742        0        1        1        0      0      1  \n",
      "3  0.000000        0        1        0        0      0      1  \n",
      "4  1.000000        1        1        1        1      1      0  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.head(5))\n",
    "print(data2.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MgIkmuDpY_2",
    "outputId": "ab38cfbb-5da6-4c08-8933-cf6a24aaf1c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2020926\n",
      "1    2020926\n",
      "Name: 15c3d_5, dtype: int64\n",
      "1    2024530\n",
      "0     776389\n",
      "Name: 15c3d_5, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['15c3d_5'].value_counts())\n",
    "print(data2['15c3d_5'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzJJ4H9jXURJ"
   },
   "source": [
    "## BUILDING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hLwTLCxo0TI",
    "outputId": "7b77fee4-ff21-4074-a284-aa65f9502109"
   },
   "outputs": [],
   "source": [
    "print(data.head())\n",
    "print(data2.head())\n",
    "print(data.info())\n",
    "print(data2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3FO73jPXXYa1"
   },
   "outputs": [],
   "source": [
    "# divide test set and training set\n",
    "\n",
    "X_train = data.drop(\"15c3d_5\", axis= 1)\n",
    "y_train = data['15c3d_5']\n",
    "\n",
    "X_test = data2.drop(\"15c3d_5\", axis=1)\n",
    "y_test = data2['15c3d_5']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCrDXOcjVoa0"
   },
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# rus = RandomUnderSampler(random_state=0)\n",
    "# rus.fit(X, y)\n",
    "# X_resampled, y_resampled = rus.sample(X, y)\n",
    "# colors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled]\n",
    "# plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, linewidth=0.5, edgecolor='black')\n",
    "# sns.despine()\n",
    "# plt.title(\"RandomUnderSampler Output ($n_{class}=64)$\")\n",
    "# pass\n",
    "\n",
    "\n",
    "\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "# # fit and apply the transform\n",
    "# X_train, y_train = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxqRPOByVqxy",
    "outputId": "379cfc87-2a0a-4e55-aa45-309f795eb3b7"
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 42)\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size= 0.2, random_state= 42)\n",
    "\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeSoxcuwXf-O"
   },
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4jzKHL4dYi55",
    "outputId": "2ecb82d1-523b-4c1e-cda7-cc2a09ed09b5"
   },
   "outputs": [],
   "source": [
    "#t5\n",
    "logreg=LogisticRegression(max_iter=10000)\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "logreg.fit(X_train, y_train) \n",
    "score = logreg.score(X_train, y_train)\n",
    "score2 = logreg.score(X_test, y_test)\n",
    "\n",
    "print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORi0eCAkYjyI"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#DT\n",
    "dt=DecisionTreeClassifier(random_state=42, max_depth = 10)\n",
    "dt.fit(X_train, y_train)\n",
    "score = dt.score(X_train, y_train)\n",
    "score2 = dt.score(X_test, y_test)\n",
    "\n",
    "print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJo14NFRZp4m"
   },
   "source": [
    "### XGBOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-T5T52kaLfz"
   },
   "outputs": [],
   "source": [
    "#T5\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_model.fit(X_train, y_train) \n",
    "score = xgb_model.score(X_train, y_train)\n",
    "score2 = xgb_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRn-GmBlaMLp"
   },
   "outputs": [],
   "source": [
    "# T6\n",
    "\n",
    "# xgb_model = XGBClassifier()\n",
    "# xgb_model.fit(X_train2, y_train2)\n",
    "\n",
    "# xgb_model.fit(X_train2, y_train2) \n",
    "# score = xgb_model.score(X_train2, y_train2)\n",
    "# score2 = xgb_model.score(X_test2, y_test2)\n",
    "\n",
    "# print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "# print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "# y_pred2 = xgb_model.predict(X_test2)\n",
    "# print(confusion_matrix(y_test2,y_pred2))\n",
    "# print(classification_report(y_test2,y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQcGrkGyarD-"
   },
   "source": [
    "### GRADIENT BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7wJhkVIav-0"
   },
   "outputs": [],
   "source": [
    "#t5\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=125,max_depth=5)\n",
    "gb.fit(X_train, y_train) \n",
    "score = gb.score(X_train, y_train)\n",
    "score2 = gb.score(X_test, y_test)\n",
    "\n",
    "print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "y_pred = gb.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3tKj4MZawOZ"
   },
   "outputs": [],
   "source": [
    "#t6\n",
    "\n",
    "# gb = GradientBoostingClassifier(n_estimators=125,max_depth=5)\n",
    "# gb.fit(X_train2, y_train2) \n",
    "# score = gb.score(X_train2, y_train2)\n",
    "# score2 = gb.score(X_test2, y_test2)\n",
    "\n",
    "# print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "# print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "# y_pred2 = gb.predict(X_test2)\n",
    "# print(confusion_matrix(y_test2,y_pred2))\n",
    "# print(classification_report(y_test2,y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7JaFnIOb3HQ"
   },
   "source": [
    "### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MLqnb-fb5y9"
   },
   "outputs": [],
   "source": [
    "#t5\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1200, max_depth=30, n_jobs =-1, min_samples_leaf=1,\n",
    "                                min_samples_split= 10, max_features = 'auto', criterion = 'entropy', bootstrap= True,\n",
    "                              random_state=42)\n",
    "rf.fit(X_train, y_train) \n",
    "score = rf.score(X_train, y_train)\n",
    "score2 = rf.score(X_test, y_test)\n",
    "\n",
    "print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4voo9LWKb6Eu"
   },
   "outputs": [],
   "source": [
    "#t6\n",
    "\n",
    "# rf = RandomForestClassifier(n_estimators=1200, max_depth=80, n_jobs =-1, min_samples_leaf=1,\n",
    "#                                 min_samples_split= 10, max_features = 'auto', criterion = 'entropy', bootstrap= True,\n",
    "#                               random_state=42)\n",
    "# rf.fit(X_train2, y_train2) \n",
    "# score = rf.score(X_train2, y_train2)\n",
    "# score2 = rf.score(X_test2, y_test2)\n",
    "\n",
    "# print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "# print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "# y_pred2 = rf.predict(X_test2)\n",
    "# print(confusion_matrix(y_test2,y_pred2))\n",
    "# print(classification_report(y_test2,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train,y_train = shuffle(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qawiSiJddQXm"
   },
   "source": [
    "### DEEP NEURAL DECISION FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9XAEQ8WAdX1C"
   },
   "outputs": [],
   "source": [
    "data_train = pd.concat([X_train, y_train], axis=1)\n",
    "data_train['15c3d_5'] = data_train['15c3d_5'].astype(str)\n",
    "data_train['15c3d_5'].replace([\"0\", \"1\"], [\"y\", \"n\"], inplace= True)\n",
    "\n",
    "data_test = pd.concat([X_test, y_test], axis=1)\n",
    "data_test['15c3d_5'] = data_test['15c3d_5'].astype(str)\n",
    "data_test['15c3d_5'].replace([\"0\", \"1\"], [\"y\", \"n\"], inplace= True)\n",
    "\n",
    "# data_train2 = pd.concat([X_train2, y_train2], axis=1)\n",
    "# data_train2['15c3d_6'] = data_train2['15c3d_6'].astype(str)\n",
    "# data_train2['15c3d_6'].replace([\"0\", \"1\"], [\"y\", \"n\"], inplace= True)\n",
    "\n",
    "# data_test2 = pd.concat([X_test2, y_test2], axis=1)\n",
    "# data_test2['15c3d_6'] = data_test2['15c3d_6'].astype(str)\n",
    "# data_test2['15c3d_6'].replace([\"0\", \"1\"], [\"y\", \"n\"], inplace= True)\n",
    "\n",
    "data_train.to_csv(\"dndf_smote.csv\", index=False, header=False)\n",
    "data_test.to_csv(\"dndf_test.csv\", index=False, header=False)\n",
    "\n",
    "# data_train2.to_csv(\"train2.csv\", index=False, header=False)\n",
    "# data_test2.to_csv(\"test2.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7WZ_unlmgjUL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              AGE      TD_2      TD_3      TD_4    DATA_2    DATA_3    DATA_4  \\\n",
      "1851691  0.332959  0.010322  0.005224  0.011563  0.000000  0.000000  0.000000   \n",
      "1430167  0.071081  0.013746  0.014492  0.014934  0.031804  0.042654  0.043359   \n",
      "3304565  0.039282  0.000000  0.000000  0.002573  0.000000  0.000000  0.002917   \n",
      "97575    0.251403  0.000000  0.000000  0.001543  0.000000  0.000000  0.000192   \n",
      "3397424  0.173588  0.003682  0.010385  0.008733  0.010287  0.015424  0.009726   \n",
      "\n",
      "         ONNET_IN_2  ONNET_IN_3  ONNET_IN_4  ...  RC_MONEY_4  REG_ON_2  \\\n",
      "1851691    0.000877    0.001546    0.001389  ...    0.000000  1.000000   \n",
      "1430167    0.004984    0.006900    0.005765  ...    0.040000  1.000000   \n",
      "3304565    0.000000    0.000000    0.000000  ...    0.006667  0.000000   \n",
      "97575      0.000000    0.000000    0.000030  ...    0.000000  0.000000   \n",
      "3397424    0.000995    0.002687    0.004601  ...    0.022667  0.892857   \n",
      "\n",
      "         REG_ON_3  REG_ON_4  15c3d_2  15c3d_3  15c3d_4  OS_FF  OS_SM  15c3d_5  \n",
      "1851691  1.000000  1.000000        1        1        1      1      0        n  \n",
      "1430167  1.000000  1.000000        1        1        1      0      1        n  \n",
      "3304565  0.000000  0.733333        0        0        1      0      1        y  \n",
      "97575    0.032258  0.900000        0        0        1      0      1        n  \n",
      "3397424  0.935484  0.666667        1        1        1      0      0        y  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_train.head())\n",
    "# print(data_test2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_wp502_4dYY8"
   },
   "outputs": [],
   "source": [
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = data_train.drop(\"15c3d_5\", axis=1).columns\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \n",
    "}\n",
    "# A list of the columns to ignore from the dataset.\n",
    "#IGNORE_COLUMN_NAMES = [\"fnlwgt\"]\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = []\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES else [\"NA\"]\n",
    "    for feature_name in data_train.columns\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"15c3d_5\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\"y\", \"n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BvabIk0mdYsE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nckh1/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py:2446: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "target_label_lookup = StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=data_train.columns,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        na_value=\" \",\n",
    "        shuffle=shuffle,\n",
    "    ).map(lambda features, target: (features, target_label_lookup(target)))\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Dq8UvumOdYuu"
   },
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "q428nXfFdYyk"
   },
   "outputs": [],
   "source": [
    "def encode_inputs(inputs):\n",
    "    encoded_features = []\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "            # Create a lookup to convert a string values to an integer indices.\n",
    "            # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "            lookup = StringLookup(\n",
    "                vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n",
    "            )\n",
    "            # Convert the string input values into integer indices.\n",
    "            value_index = lookup(inputs[feature_name])\n",
    "            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims\n",
    "            )\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_feature = embedding(value_index)\n",
    "        else:\n",
    "            # Use the numerical features as-is.\n",
    "            encoded_feature = inputs[feature_name]\n",
    "            if inputs[feature_name].shape[-1] is None:\n",
    "                encoded_feature = tf.expand_dims(encoded_feature, -1)\n",
    "\n",
    "        encoded_features.append(encoded_feature)\n",
    "\n",
    "    encoded_features = layers.concatenate(encoded_features)\n",
    "    return encoded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2th6VHN1fuHM"
   },
   "outputs": [],
   "source": [
    "class NeuralDecisionTree(keras.Model):\n",
    "    def __init__(self, depth, num_features, used_features_rate, num_classes):\n",
    "        super(NeuralDecisionTree, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.num_leaves = 2 ** depth\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Create a mask for the randomly selected features.\n",
    "        num_used_features = int(num_features * used_features_rate)\n",
    "        one_hot = np.eye(num_features)\n",
    "        sampled_feature_indicies = np.random.choice(\n",
    "            np.arange(num_features), num_used_features, replace=False\n",
    "        )\n",
    "        self.used_features_mask = one_hot[sampled_feature_indicies]\n",
    "\n",
    "        # Initialize the weights of the classes in leaves.\n",
    "        self.pi = tf.Variable(\n",
    "            initial_value=tf.random_normal_initializer()(\n",
    "                shape=[self.num_leaves, self.num_classes]\n",
    "            ),\n",
    "            dtype=\"float32\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # Initialize the stochastic routing layer.\n",
    "        self.decision_fn = layers.Dense(\n",
    "            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n",
    "        )\n",
    "\n",
    "    def call(self, features):\n",
    "        batch_size = tf.shape(features)[0]\n",
    "\n",
    "        # Apply the feature mask to the input features.\n",
    "        features = tf.matmul(\n",
    "            features, self.used_features_mask, transpose_b=True\n",
    "        )  # [batch_size, num_used_features]\n",
    "        # Compute the routing probabilities.\n",
    "        decisions = tf.expand_dims(\n",
    "            self.decision_fn(features), axis=2\n",
    "        )  # [batch_size, num_leaves, 1]\n",
    "        # Concatenate the routing probabilities with their complements.\n",
    "        decisions = layers.concatenate(\n",
    "            [decisions, 1 - decisions], axis=2\n",
    "        )  # [batch_size, num_leaves, 2]\n",
    "\n",
    "        mu = tf.ones([batch_size, 1, 1])\n",
    "\n",
    "        begin_idx = 1\n",
    "        end_idx = 2\n",
    "        # Traverse the tree in breadth-first order.\n",
    "        for level in range(self.depth):\n",
    "            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n",
    "            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n",
    "            level_decisions = decisions[\n",
    "                :, begin_idx:end_idx, :\n",
    "            ]  # [batch_size, 2 ** level, 2]\n",
    "            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n",
    "            begin_idx = end_idx\n",
    "            end_idx = begin_idx + 2 ** (level + 1)\n",
    "\n",
    "        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n",
    "        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n",
    "        outputs = tf.matmul(mu, probabilities)  # [batch_size, num_classes]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "I8p-tFlpfuKN"
   },
   "outputs": [],
   "source": [
    "class NeuralDecisionForest(keras.Model):\n",
    "    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n",
    "        super(NeuralDecisionForest, self).__init__()\n",
    "        self.ensemble = []\n",
    "        # Initialize the ensemble by adding NeuralDecisionTree instances.\n",
    "        # Each tree will have its own randomly selected input features to use.\n",
    "        for _ in range(num_trees):\n",
    "            self.ensemble.append(\n",
    "                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
    "            )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = tf.zeros([batch_size, num_classes])\n",
    "\n",
    "        # Aggregate the outputs of trees in the ensemble.\n",
    "        for tree in self.ensemble:\n",
    "            outputs += tree(inputs)\n",
    "        # Divide the outputs by the ensemble size to get the average.\n",
    "        outputs /= len(self.ensemble)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8WMGC2wRfuOI"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "def run_experiment(model):\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    train_dataset = get_dataset_from_csv(\n",
    "        \"dndf_smote.csv\", shuffle=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    model.fit(train_dataset, epochs=num_epochs)\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    print(\"Evaluating the model on the test data...\")\n",
    "    test_dataset = get_dataset_from_csv(\"dndf_test.csv\", batch_size=batch_size)\n",
    "\n",
    "    _, accuracy = model.evaluate(test_dataset)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "XxNZ1mwdgCms"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 15:25:45.469307: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31577/31577 [==============================] - 340s 10ms/step - loss: 0.3748 - sparse_categorical_accuracy: 0.8326\n",
      "Epoch 2/5\n",
      "31577/31577 [==============================] - 226s 7ms/step - loss: 0.3589 - sparse_categorical_accuracy: 0.8404\n",
      "Epoch 3/5\n",
      "31577/31577 [==============================] - 200s 6ms/step - loss: 0.3537 - sparse_categorical_accuracy: 0.8430\n",
      "Epoch 4/5\n",
      "31577/31577 [==============================] - 197s 6ms/step - loss: 0.3504 - sparse_categorical_accuracy: 0.8448\n",
      "Epoch 5/5\n",
      "31577/31577 [==============================] - 198s 6ms/step - loss: 0.3476 - sparse_categorical_accuracy: 0.8461\n",
      "Model training finished\n",
      "Evaluating the model on the test data...\n",
      "21883/21883 [==============================] - 174s 8ms/step - loss: 0.5356 - sparse_categorical_accuracy: 0.7502\n",
      "Test accuracy: 75.02%\n"
     ]
    }
   ],
   "source": [
    "num_trees = 25\n",
    "depth = 5\n",
    "used_features_rate = 0.5\n",
    "num_classes = len(TARGET_LABELS)\n",
    "\n",
    "\n",
    "def create_forest_model():\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "#     features = layers.Dense(32,  activation ='tanh'\n",
    "#                )(features)\n",
    "    features = layers.Dense(16, activation='relu')(features)\n",
    "#     features = layers.BatchNormalization()(features)\n",
    "    num_features = features.shape[1]\n",
    "\n",
    "    forest_model = NeuralDecisionForest(\n",
    "        num_trees, depth, num_features, used_features_rate, num_classes\n",
    "    )\n",
    "\n",
    "    outputs = forest_model(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "forest_model = create_forest_model()\n",
    "\n",
    "run_experiment(forest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.83      0.65    776389\n",
      "           1       0.92      0.72      0.81   2024530\n",
      "\n",
      "    accuracy                           0.75   2800919\n",
      "   macro avg       0.72      0.78      0.73   2800919\n",
      "weighted avg       0.81      0.75      0.76   2800919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = get_dataset_from_csv(\"dndf_test.csv\", batch_size=batch_size)\n",
    "aa = forest_model.predict(test_dataset)\n",
    "y_pred = []\n",
    "i = 0\n",
    "for a in aa:\n",
    "  if a[0] > a[1]:\n",
    "    y_pred.append(0)\n",
    "  else:\n",
    "    y_pred.append(1)\n",
    "  i = i + 1\n",
    "y_pred = np.array(y_pred)\n",
    "print(y_pred)\n",
    "y_test = data_test['15c3d_5']\n",
    "y_test.replace([\"y\", \"n\"], [\"0\", \"1\"], inplace= True)\n",
    "y_test = y_test.astype(np.int64)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bản sao của Bản sao của T+1_smote.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
