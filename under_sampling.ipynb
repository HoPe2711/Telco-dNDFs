{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c911a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac5b5c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sub_model_15c3d_t5.csv')\n",
    "\n",
    "data2 = pd.read_csv('sub_model_15c3d_t6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1eda15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,6):\n",
    "  data = data[data['RC_MONEY_' + str(i)] >= 0]\n",
    "  data2 = data2[data2['RC_MONEY_' + str(i+1)] >= 0]\n",
    "data['OS'].fillna(\"Unknow\", inplace = True)\n",
    "data2['OS'].fillna(\"Unknow\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5875ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data)\n",
    "data2 = pd.get_dummies(data2)\n",
    "\n",
    "data.drop('OS_Unknow', axis = 1, inplace = True)\n",
    "data2.drop('OS_Unknow', axis =1, inplace = True)\n",
    "data.drop('PRODUCT_CODE', axis= 1, inplace= True)\n",
    "data.drop('PROVINCE', axis= 1, inplace= True)\n",
    "data.drop('TD_5', axis= 1, inplace= True)\n",
    "data.drop('DATA_5', axis= 1, inplace= True)\n",
    "data.drop('ONNET_IN_5', axis= 1, inplace= True)\n",
    "data.drop('ONNET_OUT_5', axis= 1, inplace= True)\n",
    "data.drop('OFFNET_IN_5', axis= 1, inplace= True)\n",
    "data.drop('OFFNET_OUT_5', axis= 1, inplace= True)\n",
    "data.drop('PACK_TIME_5', axis= 1, inplace= True)\n",
    "data.drop('PACK_MONEY_5', axis= 1, inplace= True)\n",
    "data.drop('RC_TIME_5', axis= 1, inplace= True)\n",
    "data.drop('RC_MONEY_5', axis= 1, inplace= True)\n",
    "data.drop('REG_ON_5', axis= 1, inplace= True)\n",
    "\n",
    "data2.drop('PRODUCT_CODE', axis= 1, inplace= True)\n",
    "data2.drop('PROVINCE', axis= 1, inplace= True)\n",
    "data2.drop('TD_6', axis= 1, inplace= True)\n",
    "data2.drop('DATA_6', axis= 1, inplace= True)\n",
    "data2.drop('ONNET_IN_6', axis= 1, inplace= True)\n",
    "data2.drop('ONNET_OUT_6', axis= 1, inplace= True)\n",
    "data2.drop('OFFNET_IN_6', axis= 1, inplace= True)\n",
    "data2.drop('OFFNET_OUT_6', axis= 1, inplace= True)\n",
    "data2.drop('PACK_TIME_6', axis= 1, inplace= True)\n",
    "data2.drop('PACK_MONEY_6', axis= 1, inplace= True)\n",
    "data2.drop('RC_TIME_6', axis= 1, inplace= True)\n",
    "data2.drop('RC_MONEY_6', axis= 1, inplace= True)\n",
    "data2.drop('REG_ON_6', axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "033f1c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        AGE      TD_2      TD_3      TD_4    DATA_2    DATA_3    DATA_4  \\\n",
      "0  0.054994  0.000000  0.009464  0.000000  0.000276  0.004625  0.000000   \n",
      "1  0.057613  0.000715  0.001012  0.000798  0.000589  0.000013  0.000026   \n",
      "2  0.092780  0.000149  0.004604  0.000000  0.000000  0.000156  0.000000   \n",
      "3  0.049009  0.000000  0.007098  0.007718  0.000000  0.005340  0.012421   \n",
      "4  0.548822  0.000000  0.004732  0.010291  0.001541  0.003066  0.008059   \n",
      "\n",
      "   ONNET_IN_2  ONNET_IN_3  ONNET_IN_4  ...  RC_MONEY_4  REG_ON_2  REG_ON_3  \\\n",
      "0    0.000637    0.004552    0.000088  ...    0.000000  0.142857  0.935484   \n",
      "1    0.001596    0.000125    0.000115  ...    0.000000  0.285714  0.419355   \n",
      "2    0.000025    0.000007    0.000000  ...    0.000000  0.071429  0.258065   \n",
      "3    0.000000    0.003432    0.004011  ...    0.020000  0.000000  0.645161   \n",
      "4    0.000185    0.000366    0.000322  ...    0.026667  1.000000  1.000000   \n",
      "\n",
      "   REG_ON_4  15c3d_2  15c3d_3  15c3d_4  15c3d_5  OS_FF  OS_SM  \n",
      "0  0.000000        0        1        0        0      0      1  \n",
      "1  0.733333        1        1        1        0      0      1  \n",
      "2  0.066667        0        1        0        1      1      0  \n",
      "3  1.000000        0        1        1        1      0      1  \n",
      "4  1.000000        0        1        1        1      0      0  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nckh1/anaconda3/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- DATA_5\n",
      "- OFFNET_IN_5\n",
      "- OFFNET_OUT_5\n",
      "- ONNET_IN_5\n",
      "- ONNET_OUT_5\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- DATA_2\n",
      "- OFFNET_IN_2\n",
      "- OFFNET_OUT_2\n",
      "- ONNET_IN_2\n",
      "- ONNET_OUT_2\n",
      "- ...\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        AGE      TD_3      TD_4      TD_5    DATA_3    DATA_4    DATA_5  \\\n",
      "0  0.075196  0.005456  0.000473  0.002058  0.004787  0.000702  0.003118   \n",
      "1  0.329966  0.003492  0.004751  0.002573  0.002193  0.002261  0.005235   \n",
      "2  0.046016  0.000000  0.004732  0.012864  0.001291  0.002975  0.007417   \n",
      "3  0.046016  0.000000  0.002366  0.000000  0.000000  0.000637  0.000000   \n",
      "4  0.279835  0.006538  0.003365  0.005377  0.000000  0.000000  0.000000   \n",
      "\n",
      "   ONNET_IN_3  ONNET_IN_4  ONNET_IN_5  ...  RC_MONEY_5  REG_ON_3  REG_ON_4  \\\n",
      "0    0.000515    0.000017    0.000099  ...    0.006667  1.107143  0.967742   \n",
      "1    0.003647    0.001347    0.005638  ...    0.006667  1.071429  0.967742   \n",
      "2    0.000100    0.001519    0.000207  ...    0.033333  0.428571  0.967742   \n",
      "3    0.000031    0.001167    0.000000  ...    0.000000  0.214286  0.225806   \n",
      "4    0.002530    0.002235    0.001284  ...    0.013333  1.071429  0.967742   \n",
      "\n",
      "   REG_ON_5  15c3d_3  15c3d_4  15c3d_5  15c3d_6  OS_FF  OS_SM  \n",
      "0  0.933333        1        1        1        1      0      1  \n",
      "1  1.033333        1        1        1        1      0      1  \n",
      "2  1.000000        0        1        1        0      0      1  \n",
      "3  0.000000        0        1        0        0      0      1  \n",
      "4  1.033333        1        1        1        1      1      0  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data2.drop('ISDN', axis = 1, inplace = True)\n",
    "data.drop('ISDN', axis = 1, inplace = True)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "tmp = scaler.fit_transform(data[data.drop(['15c3d_5','15c3d_2','15c3d_3','15c3d_4','OS_FF','OS_SM'], axis=1).columns])\n",
    "data[data.drop(['15c3d_5','15c3d_2','15c3d_3','15c3d_4','OS_FF','OS_SM'], axis=1).columns] = tmp\n",
    "print(data.head())\n",
    "\n",
    "tmp = scaler.transform(data2[data2.drop(['15c3d_5','15c3d_6','15c3d_3','15c3d_4','OS_FF','OS_SM'], axis=1).columns])\n",
    "data2[data2.drop(['15c3d_6','15c3d_5','15c3d_3','15c3d_4','OS_FF','OS_SM'], axis=1).columns] = tmp\n",
    "\n",
    "print(data2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49553062",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data2.columns:\n",
    "  if(col[-1] == '3'):\n",
    "    data2 = data2.rename({col : col[:-1] + '2'}, axis=1)\n",
    "  if(col[-1] == '4'):\n",
    "    data2 = data2.rename({col : col[:-1] + '3'}, axis=1) \n",
    "  if(col[-1] == '5'):\n",
    "    data2 = data2.rename({col : col[:-1] + '4'}, axis=1)\n",
    "  if(col[-1] == '6'):\n",
    "    data2 = data2.rename({col : col[:-1] + '5'}, axis=1)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59b3dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.drop(\"15c3d_5\", axis= 1)\n",
    "y_train = data['15c3d_5']\n",
    "\n",
    "X_test = data2.drop(\"15c3d_5\", axis=1)\n",
    "y_test = data2['15c3d_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f0c1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "X_resampled, y_resampled = RandomUnderSampler(random_state=42).fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e1a6403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:  0.80922\n",
      "Test set accuracy:  0.81109\n",
      "[[ 597142  179247]\n",
      " [ 349864 1674666]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.77      0.69    776389\n",
      "           1       0.90      0.83      0.86   2024530\n",
      "\n",
      "    accuracy                           0.81   2800919\n",
      "   macro avg       0.77      0.80      0.78   2800919\n",
      "weighted avg       0.83      0.81      0.82   2800919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt=DecisionTreeClassifier(random_state=42, max_depth = 10)\n",
    "dt.fit(X_resampled,y_resampled)\n",
    "score = dt.score(X_resampled,y_resampled)\n",
    "score2 = dt.score(X_test, y_test)\n",
    "\n",
    "print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53cdf354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nckh1/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/nckh1/anaconda3/lib/python3.9/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:56:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nckh1/anaconda3/lib/python3.9/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:  0.81849\n",
      "Test set accuracy:  0.81472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nckh1/anaconda3/lib/python3.9/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 607147  169242]\n",
      " [ 349706 1674824]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.78      0.70    776389\n",
      "           1       0.91      0.83      0.87   2024530\n",
      "\n",
      "    accuracy                           0.81   2800919\n",
      "   macro avg       0.77      0.80      0.78   2800919\n",
      "weighted avg       0.83      0.81      0.82   2800919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_resampled,y_resampled)\n",
    "score = xgb_model.score(X_resampled,y_resampled)\n",
    "score2 = xgb_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21f9a6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:  0.81375\n",
      "Test set accuracy:  0.81394\n",
      "[[ 606776  169613]\n",
      " [ 351537 1672993]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.78      0.70    776389\n",
      "           1       0.91      0.83      0.87   2024530\n",
      "\n",
      "    accuracy                           0.81   2800919\n",
      "   macro avg       0.77      0.80      0.78   2800919\n",
      "weighted avg       0.83      0.81      0.82   2800919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=125,max_depth=5)\n",
    "gb.fit(X_resampled,y_resampled) \n",
    "score = gb.score(X_resampled,y_resampled)\n",
    "score2 = gb.score(X_test, y_test)\n",
    "\n",
    "print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "y_pred = gb.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42e9bb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:  0.80805\n",
      "Test set accuracy:  0.81053\n",
      "[[ 601837  174552]\n",
      " [ 356141 1668389]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.78      0.69    776389\n",
      "           1       0.91      0.82      0.86   2024530\n",
      "\n",
      "    accuracy                           0.81   2800919\n",
      "   macro avg       0.77      0.80      0.78   2800919\n",
      "weighted avg       0.83      0.81      0.82   2800919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10,\n",
    "                              random_state=42)\n",
    "rf.fit(X_resampled,y_resampled) \n",
    "score = rf.score(X_resampled,y_resampled)\n",
    "score2 = rf.score(X_test, y_test)\n",
    "\n",
    "print(\"Training set accuracy: \", '%.5f'%(score))\n",
    "print(\"Test set accuracy: \", '%.5f'%(score2))\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a2d00d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_resampled,y_resampled = shuffle(X_resampled,y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6642d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.concat([X_resampled,y_resampled], axis=1)\n",
    "data_train['15c3d_5'] = data_train['15c3d_5'].astype(str)\n",
    "data_train['15c3d_5'].replace([\"0\", \"1\"], [\"y\", \"n\"], inplace= True)\n",
    "\n",
    "data_test = pd.concat([X_test, y_test], axis=1)\n",
    "data_test['15c3d_5'] = data_test['15c3d_5'].astype(str)\n",
    "data_test['15c3d_5'].replace([\"0\", \"1\"], [\"y\", \"n\"], inplace= True)\n",
    "\n",
    "data_train.to_csv(\"train_under.csv\", index=False, header=False)\n",
    "data_test.to_csv(\"test_under.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49399658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = data_train.drop(\"15c3d_5\", axis=1).columns\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \n",
    "}\n",
    "# A list of the columns to ignore from the dataset.\n",
    "#IGNORE_COLUMN_NAMES = [\"fnlwgt\"]\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = []\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES else [\"NA\"]\n",
    "    for feature_name in data_train.columns\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"15c3d_5\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\"y\", \"n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be9c940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nckh1/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py:2446: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "target_label_lookup = StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=data_train.columns,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        na_value=\" \",\n",
    "        shuffle=shuffle,\n",
    "    ).map(lambda features, target: (features, target_label_lookup(target)))\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21ab7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8233262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inputs(inputs):\n",
    "    encoded_features = []\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "            # Create a lookup to convert a string values to an integer indices.\n",
    "            # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "            lookup = StringLookup(\n",
    "                vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n",
    "            )\n",
    "            # Convert the string input values into integer indices.\n",
    "            value_index = lookup(inputs[feature_name])\n",
    "            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims\n",
    "            )\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_feature = embedding(value_index)\n",
    "        else:\n",
    "            # Use the numerical features as-is.\n",
    "            encoded_feature = inputs[feature_name]\n",
    "            if inputs[feature_name].shape[-1] is None:\n",
    "                encoded_feature = tf.expand_dims(encoded_feature, -1)\n",
    "\n",
    "        encoded_features.append(encoded_feature)\n",
    "\n",
    "    encoded_features = layers.concatenate(encoded_features)\n",
    "    return encoded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eaeff513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDecisionTree(keras.Model):\n",
    "    def __init__(self, depth, num_features, used_features_rate, num_classes):\n",
    "        super(NeuralDecisionTree, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.num_leaves = 2 ** depth\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Create a mask for the randomly selected features.\n",
    "        num_used_features = int(num_features * used_features_rate)\n",
    "        one_hot = np.eye(num_features)\n",
    "        sampled_feature_indicies = np.random.choice(\n",
    "            np.arange(num_features), num_used_features, replace=False\n",
    "        )\n",
    "        self.used_features_mask = one_hot[sampled_feature_indicies]\n",
    "\n",
    "        # Initialize the weights of the classes in leaves.\n",
    "        self.pi = tf.Variable(\n",
    "            initial_value=tf.random_normal_initializer()(\n",
    "                shape=[self.num_leaves, self.num_classes]\n",
    "            ),\n",
    "            dtype=\"float32\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # Initialize the stochastic routing layer.\n",
    "        self.decision_fn = layers.Dense(\n",
    "            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n",
    "        )\n",
    "\n",
    "    def call(self, features):\n",
    "        batch_size = tf.shape(features)[0]\n",
    "\n",
    "        # Apply the feature mask to the input features.\n",
    "        features = tf.matmul(\n",
    "            features, self.used_features_mask, transpose_b=True\n",
    "        )  # [batch_size, num_used_features]\n",
    "        # Compute the routing probabilities.\n",
    "        decisions = tf.expand_dims(\n",
    "            self.decision_fn(features), axis=2\n",
    "        )  # [batch_size, num_leaves, 1]\n",
    "        # Concatenate the routing probabilities with their complements.\n",
    "        decisions = layers.concatenate(\n",
    "            [decisions, 1 - decisions], axis=2\n",
    "        )  # [batch_size, num_leaves, 2]\n",
    "\n",
    "        mu = tf.ones([batch_size, 1, 1])\n",
    "\n",
    "        begin_idx = 1\n",
    "        end_idx = 2\n",
    "        # Traverse the tree in breadth-first order.\n",
    "        for level in range(self.depth):\n",
    "            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n",
    "            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n",
    "            level_decisions = decisions[\n",
    "                :, begin_idx:end_idx, :\n",
    "            ]  # [batch_size, 2 ** level, 2]\n",
    "            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n",
    "            begin_idx = end_idx\n",
    "            end_idx = begin_idx + 2 ** (level + 1)\n",
    "\n",
    "        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n",
    "        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n",
    "        outputs = tf.matmul(mu, probabilities)  # [batch_size, num_classes]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "95aae8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDecisionForest(keras.Model):\n",
    "    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n",
    "        super(NeuralDecisionForest, self).__init__()\n",
    "        self.ensemble = []\n",
    "        # Initialize the ensemble by adding NeuralDecisionTree instances.\n",
    "        # Each tree will have its own randomly selected input features to use.\n",
    "        for _ in range(num_trees):\n",
    "            self.ensemble.append(\n",
    "                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
    "            )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = tf.zeros([batch_size, num_classes])\n",
    "\n",
    "        # Aggregate the outputs of trees in the ensemble.\n",
    "        for tree in self.ensemble:\n",
    "            outputs += tree(inputs)\n",
    "        # Divide the outputs by the ensemble size to get the average.\n",
    "        outputs /= len(self.ensemble)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2504cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 265\n",
    "num_epochs = 10\n",
    "hidden_units = [64, 64]\n",
    "\n",
    "\n",
    "def run_experiment(model):\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    train_dataset = get_dataset_from_csv(\n",
    "        \"train_under.csv\", shuffle=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    model.fit(train_dataset, epochs=num_epochs)\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    print(\"Evaluating the model on the test data...\")\n",
    "    test_dataset = get_dataset_from_csv(\"test_under.csv\", batch_size=batch_size)\n",
    "\n",
    "    _, accuracy = model.evaluate(test_dataset)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0cc48bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/10\n",
      "5762/5762 [==============================] - 127s 22ms/step - loss: 0.4321 - sparse_categorical_accuracy: 0.7996\n",
      "Epoch 2/10\n",
      "5762/5762 [==============================] - 88s 15ms/step - loss: 0.4246 - sparse_categorical_accuracy: 0.8035\n",
      "Epoch 3/10\n",
      "5762/5762 [==============================] - 88s 15ms/step - loss: 0.4225 - sparse_categorical_accuracy: 0.8045\n",
      "Epoch 4/10\n",
      "5762/5762 [==============================] - 87s 15ms/step - loss: 0.4214 - sparse_categorical_accuracy: 0.8052\n",
      "Epoch 5/10\n",
      "5762/5762 [==============================] - 87s 15ms/step - loss: 0.4205 - sparse_categorical_accuracy: 0.8057\n",
      "Epoch 6/10\n",
      "5762/5762 [==============================] - 87s 15ms/step - loss: 0.4199 - sparse_categorical_accuracy: 0.8061\n",
      "Epoch 7/10\n",
      "5762/5762 [==============================] - 87s 15ms/step - loss: 0.4192 - sparse_categorical_accuracy: 0.8063\n",
      "Epoch 8/10\n",
      "5762/5762 [==============================] - 88s 15ms/step - loss: 0.4188 - sparse_categorical_accuracy: 0.8067\n",
      "Epoch 9/10\n",
      "5762/5762 [==============================] - 87s 15ms/step - loss: 0.4183 - sparse_categorical_accuracy: 0.8072\n",
      "Epoch 10/10\n",
      "5762/5762 [==============================] - 87s 15ms/step - loss: 0.4179 - sparse_categorical_accuracy: 0.8075\n",
      "Model training finished\n",
      "Evaluating the model on the test data...\n",
      "10570/10570 [==============================] - 131s 12ms/step - loss: 0.4136 - sparse_categorical_accuracy: 0.8066\n",
      "Test accuracy: 80.66%\n"
     ]
    }
   ],
   "source": [
    "depth = 10\n",
    "used_features_rate = 1.0\n",
    "num_classes = len(TARGET_LABELS)\n",
    "\n",
    "\n",
    "def create_tree_model():\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "    features = layers.BatchNormalization()(features)\n",
    "    num_features = features.shape[1]\n",
    "\n",
    "    tree = NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
    "\n",
    "    outputs = tree(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "tree_model = create_tree_model()\n",
    "run_experiment(tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8368da66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.80      0.70    776389\n",
      "           1       0.91      0.81      0.86   2024530\n",
      "\n",
      "    accuracy                           0.81   2800919\n",
      "   macro avg       0.76      0.80      0.78   2800919\n",
      "weighted avg       0.83      0.81      0.81   2800919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = get_dataset_from_csv(\"test_under.csv\", batch_size=batch_size)\n",
    "aa = tree_model.predict(test_dataset)\n",
    "y_pred = []\n",
    "i = 0\n",
    "for a in aa:\n",
    "  if a[0] > a[1]:\n",
    "    y_pred.append(0)\n",
    "  else:\n",
    "    y_pred.append(1)\n",
    "  i = i + 1\n",
    "y_pred = np.array(y_pred)\n",
    "print(y_pred)\n",
    "y_test = data_test['15c3d_5']\n",
    "y_test.replace([\"y\", \"n\"], [\"0\", \"1\"], inplace= True)\n",
    "y_test = y_test.astype(np.int64)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30c05f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/10\n",
      "5762/5762 [==============================] - 121s 19ms/step - loss: 0.4349 - sparse_categorical_accuracy: 0.7991\n",
      "Epoch 2/10\n",
      "5762/5762 [==============================] - 41s 7ms/step - loss: 0.4266 - sparse_categorical_accuracy: 0.8038\n",
      "Epoch 3/10\n",
      "5762/5762 [==============================] - 41s 7ms/step - loss: 0.4247 - sparse_categorical_accuracy: 0.8049\n",
      "Epoch 4/10\n",
      "5762/5762 [==============================] - 41s 7ms/step - loss: 0.4234 - sparse_categorical_accuracy: 0.8057\n",
      "Epoch 5/10\n",
      "5762/5762 [==============================] - 41s 7ms/step - loss: 0.4227 - sparse_categorical_accuracy: 0.8061\n",
      "Epoch 6/10\n",
      "5762/5762 [==============================] - 42s 7ms/step - loss: 0.4222 - sparse_categorical_accuracy: 0.8064\n",
      "Epoch 7/10\n",
      "5762/5762 [==============================] - 43s 7ms/step - loss: 0.4218 - sparse_categorical_accuracy: 0.8065\n",
      "Epoch 8/10\n",
      "5762/5762 [==============================] - 43s 7ms/step - loss: 0.4215 - sparse_categorical_accuracy: 0.8067\n",
      "Epoch 9/10\n",
      "5762/5762 [==============================] - 44s 8ms/step - loss: 0.4213 - sparse_categorical_accuracy: 0.8068\n",
      "Epoch 10/10\n",
      "5762/5762 [==============================] - 43s 7ms/step - loss: 0.4211 - sparse_categorical_accuracy: 0.8068\n",
      "Model training finished\n",
      "Evaluating the model on the test data...\n",
      "10570/10570 [==============================] - 121s 11ms/step - loss: 0.4051 - sparse_categorical_accuracy: 0.8148\n",
      "Test accuracy: 81.48%\n"
     ]
    }
   ],
   "source": [
    "num_trees = 25\n",
    "depth = 5\n",
    "used_features_rate = 0.5\n",
    "\n",
    "\n",
    "def create_forest_model():\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "    features = layers.BatchNormalization()(features)\n",
    "    num_features = features.shape[1]\n",
    "\n",
    "    forest_model = NeuralDecisionForest(\n",
    "        num_trees, depth, num_features, used_features_rate, num_classes\n",
    "    )\n",
    "\n",
    "    outputs = forest_model(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "forest_model = create_forest_model()\n",
    "\n",
    "run_experiment(forest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02f8e646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.70    776389\n",
      "           1       0.91      0.83      0.87   2024530\n",
      "\n",
      "    accuracy                           0.81   2800919\n",
      "   macro avg       0.77      0.80      0.78   2800919\n",
      "weighted avg       0.83      0.81      0.82   2800919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa = forest_model.predict(test_dataset)\n",
    "y_pred = []\n",
    "i = 0\n",
    "for a in aa:\n",
    "  if a[0] > a[1]:\n",
    "    y_pred.append(0)\n",
    "  else:\n",
    "    y_pred.append(1)\n",
    "  i = i + 1\n",
    "y_pred = np.array(y_pred)\n",
    "print(y_pred)\n",
    "y_test = data_test['15c3d_5']\n",
    "y_test.replace([\"y\", \"n\"], [\"0\", \"1\"], inplace= True)\n",
    "y_test = y_test.astype(np.int64)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e707c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
